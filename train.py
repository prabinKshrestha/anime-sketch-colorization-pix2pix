import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from config import (
    EPOCH_MODEL_SAVE_PATH,
    ITERATION_TO_SAVE_MODEL,
    L1_LAMBDA, 
    SAVE_MODEL, 
    TRAIN_EPOCHS_COUNT, 
    TRAIN_BATCH_SIZE, 
    CHECKPOINT_DISC_FILE_NAME, 
    CHECKPOINT_GEN_FILE_NAME, 
    DEVICE, 
    LEARNING_RATE, 
    LOAD_MODEL, 
    TEST_DATASET_PATH, 
    TRAIN_DATASET_PATH
)
from utils import load_checkpoint, save_checkpoint, show_examples
from dataset import AnimePairDataset
from gans.discriminator import Discriminator
from gans.generator import Generator


#####################################################################################
#####################################################################################

# Enable CuDNN benchmarking mode to optimize performance.
torch.backends.cudnn.benchmark = True

def train(epoch, discriminator, generator, data_loader, optimizer_disc, optimizer_gen, loss_function_bce, loss_function_L1, generator_scaler, discriminator_scaler):
    ''' Train discriminator and generator for single epoch'''
    # Wrap data_loader with tqdm to create a progress bar for iteration, and leaving the progress bar visible after completion.
    loop = tqdm(data_loader, leave=True, desc=f"Epoch {epoch}:")
    # Loop through the data_loader
    for indx, (x, y) in enumerate(loop):
        x = x.to(DEVICE) #load to device
        y = y.to(DEVICE) #load to device

        # Context manager for automatic mixed precision (AMP) training.
        # It helps to accelerate training and reduce memory usage. 
               
        # Train Discriminator
        with torch.cuda.amp.autocast():
            # Generate fake images using the generator
            y_fake = generator(x)
            # Compute the discriminator output for real images
            D_real = discriminator(x, y)
            # Calculate binary cross-entropy loss for real images
            D_real_loss = loss_function_bce(D_real, torch.ones_like(D_real))
            # Compute the discriminator output for fake images
            D_fake = discriminator(x, y_fake.detach())
            # Calculate binary cross-entropy loss for fake images
            D_fake_loss = loss_function_bce(D_fake, torch.zeros_like(D_fake))
            # Overall discriminator loss (average of real and fake losses)
            D_loss = (D_real_loss + D_fake_loss) / 2
            
        # Reset gradients of the discriminator's parameters
        discriminator.zero_grad()
        # Scale the computed discriminator loss and perform backpropagation
        discriminator_scaler.scale(D_loss).backward()
        # Update the optimizer parameters (weights and biases) of the discriminator
        discriminator_scaler.step(optimizer_disc)
        # Update the GradScaler to adjust the scale factor for the next iteration
        discriminator_scaler.update()

        # Train generator
        with torch.cuda.amp.autocast():
            # Compute the discriminator output for fake images generated by the generator
            D_fake = discriminator(x, y_fake)
            # Calculate binary cross-entropy loss for the generator
            G_fake_loss = loss_function_bce(D_fake, torch.ones_like(D_fake))
            # Compute the L1 loss between the generated fake images and the ground truth images
            # L1 loss encourages similarity between generated and ground truth images
            # L1_LAMBDA is a hyperparameter to control the importance of L1 loss in the overall loss
            L1 = loss_function_L1(y_fake, y) * L1_LAMBDA
            # Overall generator loss is the sum of binary cross-entropy loss and L1 loss
            G_loss = G_fake_loss + L1

        optimizer_gen.zero_grad()
        generator_scaler.scale(G_loss).backward()
        generator_scaler.step(optimizer_gen)
        generator_scaler.update()

        if indx % 10 == 0:
            loop.set_postfix(
                D_real=torch.sigmoid(D_real).mean().item(),
                D_fake=torch.sigmoid(D_fake).mean().item(),
            )

#####################################################################################
#####################################################################################

def train_pix2pix():
    '''Defines the pix2pix and then start training for desired number of epochs'''
    # - The discriminator model is initialized with specified input channels and moved to the specified device (e.g., GPU).
    # - The generator model is initialized with specified input channels and features, and also moved to the specified device.
    discriminator = Discriminator(in_channels=3).to(DEVICE)  # Initialize discriminator model
    generator = Generator(in_channels=3, features=64).to(DEVICE)  # Initialize generator model
    
    # Initialize Adam optimizers for updating the parameters of the discriminator and generator models.
    # - opt_disc is initialized to optimize the parameters of the discriminator (disc) with the specified learning rate and beta coefficients.
    optimizer_disc = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))
    # - opt_gen is initialized to optimize the parameters of the generator (gen) with the same learning rate and beta coefficients.
    optimizer_gen = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))
    
    # Import BCEWithLogitsLoss from the torch.nn module.
    # This loss function combines a sigmoid layer and the binary cross-entropy loss into one single class.
    loss_function_bce = nn.BCEWithLogitsLoss()
    # Import L1Loss from the torch.nn module.
    # This loss function computes the mean absolute error between each element in the input tensor and the target tensor.
    loss_function_L1 = nn.L1Loss()
                               
    # If LOAD_MODEL flag is set to True, load pre-trained checkpoints for the discriminator and generator models
    # along with their respective optimizers using the load_checkpoint function.
    if LOAD_MODEL:
        load_checkpoint(CHECKPOINT_DISC_FILE_NAME, discriminator, optimizer_disc, LEARNING_RATE)
        load_checkpoint(CHECKPOINT_GEN_FILE_NAME, generator, optimizer_gen, LEARNING_RATE)

    # Initialize the training dataset using the AnimePairDataset class with the specified training dataset path.
    training_dataset = AnimePairDataset(TRAIN_DATASET_PATH)
    # Create a DataLoader for the training dataset with the specified batch size, shuffling the data.
    training_loader = DataLoader(
        training_dataset,
        batch_size=TRAIN_BATCH_SIZE,
        shuffle=True,
    )
    # Initialize the test dataset using the AnimePairDataset class with the specified test dataset path.            
    test_dataset = AnimePairDataset(TEST_DATASET_PATH)
    # Finally, create a DataLoader for the test dataset with the specified batch size, shuffling the data.
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
    
    # Create GradScaler objects for the generator and discriminator models.
    # GradScaler is used in combination with PyTorch's automatic mixed precision (AMP) training
    # to dynamically adjust the scale of gradients during backward passes, allowing for more stable
    # and efficient training.
    generator_scaler = torch.cuda.amp.GradScaler()  # GradScaler for the generator model
    discriminator_scaler = torch.cuda.amp.GradScaler()  # GradScaler for the discriminator model

   # Iterate over the specified number of epochs
    for epoch in range(1, TRAIN_EPOCHS_COUNT+1):
        # Train the GAN for one epoch
        train(
            epoch=epoch,
            discriminator=discriminator,
            generator=generator,
            data_loader=training_loader,
            optimizer_disc=optimizer_disc,
            optimizer_gen=optimizer_gen,
            loss_function_L1=loss_function_L1,
            loss_function_bce=loss_function_bce,
            generator_scaler=generator_scaler,
            discriminator_scaler=discriminator_scaler,
        )

        if (epoch) % ITERATION_TO_SAVE_MODEL == 0:
            if SAVE_MODEL:
                print(f"*** Saving Model and Optimizer at Epoch: {epoch} ***")
                save_checkpoint(generator, optimizer_gen, filename=CHECKPOINT_GEN_FILE_NAME)
                save_checkpoint(discriminator, optimizer_disc, filename=CHECKPOINT_DISC_FILE_NAME)
                os.makedirs(EPOCH_MODEL_SAVE_PATH, exist_ok=True)
                torch.save(generator.state_dict(), f'{EPOCH_MODEL_SAVE_PATH}/epoch_{epoch}.pth')
                print(f"*** Save Complete ***")
            show_examples(generator, test_loader, epoch)

#####################################################################################
#####################################################################################

if __name__ == "__main__":
    print("Train module loaded.")